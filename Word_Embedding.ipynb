{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siddheshk02/SentenceBERT-/blob/main/Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SentenceBERT**"
      ],
      "metadata": {
        "id": "VBOyQa0cmxoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN0Vh5TXm4Ck",
        "outputId": "5088de38-d53f-4edb-ecef-c1571bebce3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 46.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.63.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 39.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 57.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=fab5778e7845fadc2871504e11b09defa18c7ebf7aaa68206700e19320bc4ca5\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import scipy"
      ],
      "metadata": {
        "id": "fkE3xdiOng5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "sentences = ['Web Development','Machine Learning','Criminal face detection using ML',\n",
        "'Discussion Forum',\n",
        "'Auto bill generator',\n",
        "'Website for visually impaired people',\n",
        "'Detection of Manipulated Facial Images',\n",
        "'Heart disease prediction',\n",
        "'2-D Game using unity engine',\n",
        "'Healthcare system DApp using Blockchain',\n",
        "'Long term water consumption plan for Mumbai',\n",
        "'Credit Card Fraud Detection System',\n",
        "'Women safety app',\n",
        "'Hand Gesture Calculator',\n",
        "'Age and Gender Detection',\n",
        "'Parking Management System',\n",
        "'Word Prediction System',\n",
        "'Diskless Booting',\n",
        "'Outcome Prediction For Cricket Matches',\n",
        "'Leaf disease detection system',\n",
        "'Meal Planning Website',\n",
        "'Fake news Classifier',\n",
        "'MEDBOT',\n",
        "'Image caption generator',\n",
        "'FUEL COST PREDICTION',\n",
        "'DigiSaatBaara',\n",
        "'Real time surveillance',\n",
        "'Job Recommendation System',\n",
        "'Helmet detection system',\n",
        "'Document Digitization',\n",
        "'APPLICATION FOR VOTING SYSTEM',\n",
        "'College Web Site',\n",
        "'Job Prediction',\n",
        "'Sentiment Analysis',\n",
        "'Video Conferencing',\n",
        "'Airport Management and Reservation App',\n",
        "'College Web Site',\n",
        "'Product Review Sentiment Analysis',\n",
        "'Travel Recommander',\n",
        "'Farmers Portal',\n",
        "'Doodle E- Learning using Deep Learning',\n",
        "'Dashboard for Airport Facilities Utilization',\n",
        "'Text Summarization',\n",
        "'Sign Language converter for Deaf people',\n",
        "'Skin cancer detection using ML',\n",
        "'Assessing soil parameters for rectification of Crop production and recommendation',\n",
        "'Geographical crime rate Prediction Tools',\n",
        "'Drug Target Bioactivity prediction',\n",
        "'E-commerce with a progressive application of drop shipping',\n",
        "'Developer Utility Tool',\n",
        "'Online Health Consultancy',\n",
        "'Video Summarization in Regional Language',\n",
        "'Recommendation of holistic growth of students',\n",
        "'2D Simulation of Complex data structures',\n",
        "'Competitive coding platform',\n",
        "'Prerequisite MOOC course recoomendation based on syllabus',\n",
        "'Audio Book Recommender web Application',\n",
        "'Distributed Finanaces',\n",
        "'Crowd check - over crowd detection and limiting',\n",
        "'Email Spam Filtering',\n",
        "'AI Virtual Painter',\n",
        "'Leaf Disease Detection using CNN',\n",
        "'Virtual Interactive Exhibition',\n",
        "'E-Learning',\n",
        "'Assistance for Dyslexic people',\n",
        "'Research Paper Recommendation System',\n",
        "'Student Result Management System',\n",
        "'Exploratory analysis of geolocational data',\n",
        "'Mobile App for Dog care and health Monitor',\n",
        "'An Illegal Parking Detection Platform Using Deep Learning',\n",
        "'Book Recommendation System',\n",
        "'Detection of phising website using Machine Learning',\n",
        "'Travel Buddy']\n",
        "# Each sentence is encoded as a 1-D vector with 78 columns\n",
        "sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "#print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\n",
        "\n",
        "#print('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])\n",
        "\n",
        "query = input('Enter the Topic: ')\n",
        "#query = 'Machine Learning' #@param {type: 'string'}\n",
        "\n",
        "queries = [query]\n",
        "query_embeddings = model.encode(queries)\n",
        "\n",
        "# Find the closest 10 sentences of the corpus for each query sentence based on cosine similarity\n",
        "number_top_matches = 10 #@param {type: \"number\"}\n",
        "\n",
        "print(\"Semantic Search Results\")\n",
        "\n",
        "for query, query_embedding in zip(queries, query_embeddings):\n",
        "    distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0]\n",
        "\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop 10 most similar sentences in corpus:\")\n",
        "\n",
        "    for idx, distance in results[0:number_top_matches]:\n",
        "        print(sentences[idx].strip(), \"(Cosine Score: %.4f)\" % (1-distance))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmF5M64w_Xgd",
        "outputId": "0fd5b8fd-e146-4041-c13e-1b8cbcc8f410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the Topic: Job Recommendation System\n",
            "Semantic Search Results\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: Job Recommendation System\n",
            "\n",
            "Top 10 most similar sentences in corpus:\n",
            "Job Recommendation System (Cosine Score: 1.0000)\n",
            "Job Prediction (Cosine Score: 0.9154)\n",
            "Developer Utility Tool (Cosine Score: 0.7324)\n",
            "Product Review Sentiment Analysis (Cosine Score: 0.7277)\n",
            "Research Paper Recommendation System (Cosine Score: 0.6787)\n",
            "Discussion Forum (Cosine Score: 0.6028)\n",
            "Web Development (Cosine Score: 0.5947)\n",
            "Sentiment Analysis (Cosine Score: 0.5909)\n",
            "Book Recommendation System (Cosine Score: 0.5886)\n",
            "Student Result Management System (Cosine Score: 0.5805)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ELMo**"
      ],
      "metadata": {
        "id": "zhRsEssUoM84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU5NQlhDvqv6",
        "outputId": "17c80512-21d5-40d6-ce27-b17f196e19cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['Web Development','Machine Learning','Criminal face detection using ML',\n",
        "'Discussion Forum',\n",
        "'Auto bill generator',\n",
        "'Website for visually impaired people',\n",
        "'Detection of Manipulated Facial Images',\n",
        "'Heart disease prediction',\n",
        "'2-D Game using unity engine',\n",
        "'Healthcare system DApp using Blockchain',\n",
        "'Long term water consumption plan for Mumbai',\n",
        "'Credit Card Fraud Detection System',\n",
        "'Women safety app',\n",
        "'Hand Gesture Calculator',\n",
        "'Age and Gender Detection',\n",
        "'Parking Management System',\n",
        "'Word Prediction System',\n",
        "'Diskless Booting',\n",
        "'Outcome Prediction For Cricket Matches',\n",
        "'Leaf disease detection system',\n",
        "'Meal Planning Website',\n",
        "'Fake news Classifier',\n",
        "'MEDBOT',\n",
        "'Image caption generator',\n",
        "'FUEL COST PREDICTION',\n",
        "'DigiSaatBaara',\n",
        "'Real time surveillance',\n",
        "'Job Recommendation System',\n",
        "'Helmet detection system',\n",
        "'Document Digitization',\n",
        "'APPLICATION FOR VOTING SYSTEM',\n",
        "'College Web Site',\n",
        "'Job Prediction',\n",
        "'Sentiment Analysis',\n",
        "'Video Conferencing',\n",
        "'Airport Management and Reservation App',\n",
        "'College Web Site',\n",
        "'Product Review Sentiment Analysis',\n",
        "'Travel Recommander',\n",
        "'Farmers Portal',\n",
        "'Doodle E- Learning using Deep Learning',\n",
        "'Dashboard for Airport Facilities Utilization',\n",
        "'Text Summarization',\n",
        "'Sign Language converter for Deaf people',\n",
        "'Skin cancer detection using ML',\n",
        "'Assessing soil parameters for rectification of Crop production and recommendation',\n",
        "'Geographical crime rate Prediction Tools',\n",
        "'Drug Target Bioactivity prediction',\n",
        "'E-commerce with a progressive application of drop shipping',\n",
        "'Developer Utility Tool',\n",
        "'Online Health Consultancy',\n",
        "'Video Summarization in Regional Language',\n",
        "'Recommendation of holistic growth of students',\n",
        "'2D Simulation of Complex data structures',\n",
        "'Competitive coding platform',\n",
        "'Prerequisite MOOC course recoomendation based on syllabus',\n",
        "'Audio Book Recommender web Application',\n",
        "'Distributed Finanaces',\n",
        "'Crowd check - over crowd detection and limiting',\n",
        "'Email Spam Filtering',\n",
        "'AI Virtual Painter',\n",
        "'Leaf Disease Detection using CNN',\n",
        "'Virtual Interactive Exhibition',\n",
        "'E-Learning',\n",
        "'Assistance for Dyslexic people',\n",
        "'Research Paper Recommendation System',\n",
        "'Student Result Management System',\n",
        "'Exploratory analysis of geolocational data',\n",
        "'Mobile App for Dog care and health Monitor',\n",
        "'An Illegal Parking Detection Platform Using Deep Learning',\n",
        "'Book Recommendation System',\n",
        "'Detection of phising website using Machine Learning',\n",
        "'Travel Buddy']"
      ],
      "metadata": {
        "id": "DZRni6KQuq4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNJ3Ic5TzvVp",
        "outputId": "65c1c4f3-da42-4e58-b00b-c799545f5ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Requirement already satisfied: tensorflow<1.16,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (1.14.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (1.21.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (1.44.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (0.8.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (1.15.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.16,>=1.15.0->tensorflow_text) (1.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow<1.16,>=1.15.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<1.16,>=1.15.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<1.16,>=1.15.0->tensorflow_text) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<1.16,>=1.15.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<1.16,>=1.15.0->tensorflow_text) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<1.16,>=1.15.0->tensorflow_text) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<1.16,>=1.15.0->tensorflow_text) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow<1.16,>=1.15.0->tensorflow_text) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
        "embeddings =embed(\n",
        "    sentences,\n",
        "    signature=\"default\",\n",
        "    as_dict=True)[\"default\"]\n",
        "#%%time\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  sess.run(tf.tables_initializer())\n",
        "  x = sess.run(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_HxMl7EtL6j",
        "outputId": "3cce2921-f533-4ee3-cdcf-62e10214dc7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from IPython.display import HTML\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "2RqDKC0y0Dg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_string = input('Enter your Topic  ')#@param {type:\"string\"}\n",
        "results_returned = \"10\" #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "embeddings2 = embed(\n",
        "    [search_string],\n",
        "    signature=\"default\",\n",
        "    as_dict=True)[\"default\"]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  sess.run(tf.tables_initializer())\n",
        "  search_vect = sess.run(embeddings2)\n",
        "  \n",
        "\n",
        "cosine_similarities = pd.Series(cosine_similarity(search_vect, x).flatten())\n",
        "output =\"\"\n",
        "for i,j in cosine_similarities.nlargest(int(results_returned)).iteritems():\n",
        "  output +='<p style=\"font-family:verdana; font-size:110%;\"> '\n",
        "  for i in sentences[i].split():\n",
        "    if i.lower() in search_string:\n",
        "      output += \" <b>\"+str(i)+\"</b>\"\n",
        "    else:\n",
        "      output += \" \"+str(i)\n",
        "  output += \"</p><hr>\"\n",
        "    \n",
        "output = '<h3>Results:</h3>'+output\n",
        "display(HTML(output))\n",
        "#   print(sentences[i])\n",
        "#   print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "MI0fCC0XvOls",
        "outputId": "006401a9-62c2-409c-d242-d32aefc10b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Topic  Job Recommendation System\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>Results:</h3><p style=\"font-family:verdana; font-size:110%;\">  Job Recommendation System</p><hr><p style=\"font-family:verdana; font-size:110%;\">  Job Prediction</p><hr><p style=\"font-family:verdana; font-size:110%;\">  Book Recommendation System</p><hr><p style=\"font-family:verdana; font-size:110%;\">  Research Paper Recommendation System</p><hr><p style=\"font-family:verdana; font-size:110%;\">  Student Result Management System</p><hr><p style=\"font-family:verdana; font-size:110%;\">  Word Prediction System</p><hr><p style=\"font-family:verdana; font-size:110%;\">  Credit Card Fraud Detection System</p><hr><p style=\"font-family:verdana; font-size:110%;\">  Product Review Sentiment Analysis</p><hr><p style=\"font-family:verdana; font-size:110%;\">  Parking Management System</p><hr><p style=\"font-family:verdana; font-size:110%;\">  Audio Book Recommender web Application</p><hr>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Doc2Vec**"
      ],
      "metadata": {
        "id": "6gET9YhKHR_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "Tdvti1xFHVxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-mKy243Oe2E",
        "outputId": "0a70f530-ebfd-42e8-a6dc-b97f25404503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"Image classification using Machine Learning.\",\n",
        "        \"Website for coders\",\n",
        "        \"Chatbot for Doubt solving\",\n",
        "        \"they chat amagingly well\"]\n",
        "\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]"
      ],
      "metadata": {
        "id": "20WCvWPUOJV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 100\n",
        "vec_size = 20\n",
        "alpha = 0.025\n",
        "\n",
        "model = Doc2Vec(size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm =1)\n",
        "  \n",
        "model.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    #print('iteration {0}'.format(epoch))\n",
        "    model.train(tagged_data,\n",
        "                total_examples=model.corpus_count,\n",
        "                epochs=model.iter)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "\n",
        "model.save(\"d2v.model\")\n",
        "#print(\"Model Saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-1F1PvyQTj9",
        "outputId": "9f4a937d-4958-432a-d14f-f53541eb7cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
            "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec\n",
        "\n",
        "def output_sentences(most_similar):\n",
        "    for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(most_similar)//2), ('LEAST', len(most_similar) - 1)]:\n",
        "      print(u'%s %s: %s\\n' % (label, most_similar[index][1], data[int(most_similar[index][0])]))\n",
        "\n",
        "model= Doc2Vec.load(\"d2v.model\")\n",
        "#to find the vector of a document which is not in training data\n",
        "str = input(\"Enter a the topic: \")\n",
        "test_data = word_tokenize(str.lower())\n",
        "v1 = model.infer_vector(test_data)\n",
        "#print(\"V1_infer\", v1)\n",
        "similar_doc = model.docvecs.most_similar([v1])\n",
        "#print(similar_doc)\n",
        "output_sentences(similar_doc) \n",
        "#print(model.docvecs['1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp4s_LobOisV",
        "outputId": "ec98cd69-9ea3-475a-d8da-77f837927c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a the topic: Frequently asked question website for students \n",
            "MOST 0.14238883554935455: Website for coders\n",
            "\n",
            "SECOND-MOST 0.02816746011376381: Chatbot for Doubt solving\n",
            "\n",
            "MEDIAN 0.013265486806631088: Image classification using Machine Learning.\n",
            "\n",
            "LEAST 0.005103684961795807: they chat amagingly well\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[int(similar_doc[0][0])])\n",
        "print(data[int(similar_doc[0][1])])\n",
        "print(data[int(similar_doc[1][0])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JyNNq0VNTeH",
        "outputId": "85c1eb81-f28b-4f33-b6d9-7dcbb785d802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "they chat amagingly well\n",
            "I love machine learning. Its awesome.\n",
            "I love coding in python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-MzD1bwfNUWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GloVe**"
      ],
      "metadata": {
        "id": "nb1BIp3CpSqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir encoder\n",
        "! curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\n",
        "  \n",
        "! mkdir GloVe\n",
        "! curl -Lo GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "! unzip GloVe/glove.840B.300d.zip -d GloVe/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyw2ifzQ8YGn",
        "outputId": "179c014c-44b3-4ff0-a173-ca015937264a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  146M  100  146M    0     0  61.8M      0  0:00:02  0:00:02 --:--:-- 61.8M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0   315    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0   352    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 2075M  100 2075M    0     0  5177k      0  0:06:50  0:06:50 --:--:-- 5455k\n",
            "Archive:  GloVe/glove.840B.300d.zip\n",
            "  inflating: GloVe/glove.840B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import spatial\n",
        "import numpy as np\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7msujR6VpUnG",
        "outputId": "c88724cd-6215-45db-a440-e87cb397daa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/encoder/')"
      ],
      "metadata": {
        "id": "QJO_Wyd18fAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lQGqc-y9-WsL",
        "outputId": "d4d53015-b4c5-4284-879a-100901ba65c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/encoder'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d6_6cLlB0wl",
        "outputId": "2c339303-522d-4138-e8e8-61fdccfe3bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting models\n",
            "  Downloading models-0.9.3.tar.gz (16 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/92/3c/ac1ddde60c02b5a46993bd3c6f4c66a9dbc100059da8333178ce17a22db5/models-0.9.3.tar.gz#sha256=b5aa29c6b57a667cda667dd9fbd33bbd15c14cc285e57dda64f4f4c0fd35e0ae (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading models-0.9.2.tar.gz (16 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/44/3a/8cd3c08995d89e87e74a9aa04784dcb9ade25c03250c281376ce4942d7f5/models-0.9.2.tar.gz#sha256=308be4d5cb707c63f967c591111a8675fe3eaa59c04516e5f6c7fff5e026ece0 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading models-0.9.1.tar.gz (16 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/10/1d/dd6ae3703251520729a9203ce95c436837ca28e0ee6794538efb136ac0e4/models-0.9.1.tar.gz#sha256=78eb7e8cef14864cda9ec5c1911f3c0771671734c71945ec4f908c505d3be8f9 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading models-0.9.0.tar.gz (14 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/05/2a/025b493fac59ba30b4ba63c15e72200b0d39a44f22a226f3d69b176a03df/models-0.9.0.tar.gz#sha256=9c16463aef8fe05e856d74c084035bac1d53936f6274444611eee7cb2ccc4556 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading models-0.8.0.tar.gz (14 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/63/e1/b6f02f5df8e27da6a3ea3b7f61ff7ab2d17b4b98290a54799c1bd712b883/models-0.8.0.tar.gz#sha256=c2d01e2c83fb39866ad622e0dbe0e211cf694b5148550cca079411249171c747 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading models-0.7.0.tar.gz (14 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/09/88/72909bdb09041e6adcc90944f8e9a08fb578aaa6e7699e80a9c147ff7c1f/models-0.7.0.tar.gz#sha256=e3d01ad00a6ec6831de6138a3f986c2efb7bd9355d8bb3f5b700eb2e96b6e3e7 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading models-0.4.0.tar.gz (11 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/79/44/59bdda622faaec86c9db3f85270e29a3485936753b779c8210d3c0499ccb/models-0.4.0.tar.gz#sha256=80314c00ee1e8b19b4536107daac00b4e986f048e58d73530a00348322bc4399 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading models-0.3.0.tar.gz (11 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/fb/dc/fd2068f4cc4f275a1a2d41a2ca6a6b3a0d49c7e86ba3a30d2fbf15731f7a/models-0.3.0.tar.gz#sha256=ce0350c757e27dcf7476d9a3f030a3f408c0b318a6b1153a83db8206766fae10 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading models-0.2.0.tar.gz (11 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/27/62/62ebd85760e4f812753056b4763f0a866cdcbc08ee6ccc2b81a2f5fb2202/models-0.2.0.tar.gz#sha256=2f5982ef56afe9239f81f5d6a461823e0dc267c6212c06754a0863b98dad60a4 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading models-0.1.1.tar.gz (11 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/ca/81/53baac8e352f956a23568f285a72bcd2d8cc04321022f0196de5eaf5e5e4/models-0.1.1.tar.gz#sha256=b117db9acd5a232f4333a5927385320826168514364e86d1dbaba44b01fab1a9 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading models-0.1.0.tar.gz (11 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/ad/3b/596d1bb079039dc29b92692e59b87d7f352a15e25cca6b5b304ff3efce9a/models-0.1.0.tar.gz#sha256=0b74111b909079ef6baaa10e1619ff5aaeaace807f0bcc948650c38e81534203 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading models-0.0.5.tar.gz (6.4 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/b9/12/3a057a76ef4063e25584e922228e82f9fb0ab20e888e97dd4d109d017207/models-0.0.5.tar.gz#sha256=5832d3221cce8918872873b7aa8436d80c829de1b7ba4ec21944c9c361a12386 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading models-0.0.4.tar.gz (6.1 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/e6/6a/0a4cb29c5d32ddbe06080deebd1e4f8896f6de56f954d690d492e8044b91/models-0.0.4.tar.gz#sha256=4537a1b50b1efb54ff52762f6108be8afcfc194927380d948d7d983ac2557a98 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading models-0.0.3.tar.gz (6.2 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/68/35/b990e771f13982d98385774c651c0c90baec4b4ef19c752b5e6add126d3f/models-0.0.3.tar.gz#sha256=9bb0315231b46d53df62c63321c33fc6935ca58db629e3aef09c735b781a9d8b (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement models (from versions: 0.0.3, 0.0.4, 0.0.5, 0.1.0, 0.1.1, 0.2.0, 0.3.0, 0.4.0, 0.7.0, 0.8.0, 0.9.0, 0.9.1, 0.9.2, 0.9.3)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for models\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models import InferSent\n",
        "\n",
        "import torch\n",
        "\n",
        "V = 2\n",
        "MODEL_PATH = '/content/encoder/infersent%s.pkl' % V #/content/encoder/infersent2.pkl\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
        "model = InferSent(params_model)\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "W2V_PATH = '/content/GloVe/glove.840B.300d.txt'\n",
        "model.set_w2v_path(W2V_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "o-BxuDkW3mw-",
        "outputId": "ee3afe9b-b03b-4a9d-c206-dcb064fa9f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-82e7344d7fc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInferSent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4yutK8kx6OhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences= ['Web Development','Machine Learning','Criminal face detection using ML',\n",
        "'Discussion Forum',\n",
        "'Auto bill generator',\n",
        "'Website for visually impaired people',\n",
        "'Detection of Manipulated Facial Images',\n",
        "'Heart disease prediction',\n",
        "'2-D Game using unity engine',\n",
        "'Healthcare system DApp using Blockchain',\n",
        "'Long term water consumption plan for Mumbai',\n",
        "'Credit Card Fraud Detection System',\n",
        "'Women safety app',\n",
        "'Hand Gesture Calculator',\n",
        "'Age and Gender Detection',\n",
        "'Parking Management System',\n",
        "'Word Prediction System',\n",
        "'Diskless Booting',\n",
        "'Outcome Prediction For Cricket Matches',\n",
        "'Leaf disease detection system',\n",
        "'Meal Planning Website',\n",
        "'Fake news Classifier',\n",
        "'MEDBOT',\n",
        "'Image caption generator',\n",
        "'FUEL COST PREDICTION',\n",
        "'DigiSaatBaara',\n",
        "'Real time surveillance',\n",
        "'Job Recommendation System',\n",
        "'Helmet detection system',\n",
        "'Document Digitization',\n",
        "'APPLICATION FOR VOTING SYSTEM',\n",
        "'College Web Site',\n",
        "'Job Prediction',\n",
        "'Sentiment Analysis',\n",
        "'Video Conferencing',\n",
        "'Airport Management and Reservation App',\n",
        "'College Web Site',\n",
        "'Product Review Sentiment Analysis',\n",
        "'Travel Recommander',\n",
        "'Farmers Portal',\n",
        "'Doodle E- Learning using Deep Learning',\n",
        "'Dashboard for Airport Facilities Utilization',\n",
        "'Text Summarization',\n",
        "'Sign Language converter for Deaf people',\n",
        "'Skin cancer detection using ML',\n",
        "'Assessing soil parameters for rectification of Crop production and recommendation',\n",
        "'Geographical crime rate Prediction Tools',\n",
        "'Drug Target Bioactivity prediction',\n",
        "'E-commerce with a progressive application of drop shipping',\n",
        "'Developer Utility Tool',\n",
        "'Online Health Consultancy',\n",
        "'Video Summarization in Regional Language',\n",
        "'Recommendation of holistic growth of students',\n",
        "'2D Simulation of Complex data structures',\n",
        "'Competitive coding platform',\n",
        "'Prerequisite MOOC course recoomendation based on syllabus',\n",
        "'Audio Book Recommender web Application',\n",
        "'Distributed Finanaces',\n",
        "'Crowd check - over crowd detection and limiting',\n",
        "'Email Spam Filtering',\n",
        "'AI Virtual Painter',\n",
        "'Leaf Disease Detection using CNN',\n",
        "'Virtual Interactive Exhibition',\n",
        "'E-Learning',\n",
        "'Assistance for Dyslexic people',\n",
        "'Research Paper Recommendation System',\n",
        "'Student Result Management System',\n",
        "'Exploratory analysis of geolocational data',\n",
        "'Mobile App for Dog care and health Monitor',\n",
        "'An Illegal Parking Detection Platform Using Deep Learning',\n",
        "'Book Recommendation System',\n",
        "'Detection of phising website using Machine Learning',\n",
        "'Travel Buddy']"
      ],
      "metadata": {
        "id": "PmDg-lHfBEpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.build_vocab(sentences, tokenize=True)\n",
        "query = \"Auto bill generator\"\n",
        "\n",
        "query_vec = model.encode(query)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "QXKUXLIo8w6F",
        "outputId": "83a6da12-5e02-4857-a397-ca19951cf6d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-fa4ba4cd9300>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Auto bill generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mquery_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, documents, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[1;32m   1161\u001b[0m             \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m             \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m         )\n\u001b[1;32m   1164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, documents, corpus_file, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTaggedLineDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         logger.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, documents, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m                     logger.warning(\n\u001b[1;32m   1290\u001b[0m                         \u001b[0;34m\"Each 'words' should be a list of words (usually unicode strings). \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'words'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = []\n",
        "for sent in sentences:\n",
        "  sim = 1- spatial.distance.cosine(query_vec, model.encode([sent])[0])\n",
        "  print( model.encode([sent])[0])\n",
        "  print(\"Sentence = \", sent, \"; similarity = \", sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "NPGYYs7K86nR",
        "outputId": "218981ff-cf7e-4775-ca42-e3029f02d55a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-5b6094f06d8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sentence = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"; similarity = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'query_vec' is not defined"
          ]
        }
      ]
    }
  ]
}